{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook works on extracting unique named entities and organizations from KDD papers and passing them into a list.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import subprocess\n",
    "import unicodedata\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk import Tree\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import polyglot\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path        = os.path.abspath(os.getcwd())\n",
    "TESTDIR     = os.path.normpath(os.path.join(os.path.expanduser(\"~\"),\"projects\",\"LC3-Creations\", \"examples\",\"KDDsample\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "I experienced unicode problems early on.  Everytime I had an error, I scoured the internet for solutions. Here's the credit.\n",
    "\n",
    "\n",
    "\n",
    "- For Typeerror codes using subprocess to convert pdf2txt output to straight unicode --> http://stackoverflow.com/questions/33283603/python-popen-communicate-str-encodeencoding-utf-8-errors-ignore-cr\n",
    "- For problems with ASCII characters --> http://stackoverflow.com/questions/175240/how-do-i-convert-a-files-format-from-unicode-to-ascii-using-python\n",
    "- For unicode characters left in unicode converted to a string  --> http://stackoverflow.com/questions/8689795/how-can-i-remove-non-ascii-characters-but-leave-periods-and-spaces-using-python\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "a = unicode(subprocess.check_output(['pdf2txt.py',str(os.path.normpath(os.path.join(TESTDIR,\"p29.pdf\")))]),errors='ignore')\n",
    "document = filter(lambda x: x in string.printable,unicodedata.normalize('NFKD', a).encode('ascii','ignore').decode('unicode_escape').encode('ascii','ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"returns named entity chunks in a given text\"\n",
    "tagged = nltk.pos_tag(nltk.word_tokenize(re.sub('[\\s]',\" \", document)))\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "# Another entity extractor\n",
    "st = StanfordNERTagger('/Users/linwood/stanford-corenlp-full-2015-04-20/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "       '/Users/linwood/stanford-corenlp-full-2015-04-20/stanford-corenlp-3.5.2.jar',\n",
    "       encoding='utf-8')\n",
    "tokenized_text = word_tokenize(re.sub('[\\s]',\" \", document))\n",
    "stanentities = st.tag(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Estimating',\n",
       " 'Local',\n",
       " 'Intrinsic',\n",
       " 'Dimensionality',\n",
       " 'Laurent',\n",
       " 'Amsaleg',\n",
       " 'Equipe',\n",
       " 'LINKMEDIA',\n",
       " ',',\n",
       " 'CNRS/IRISA',\n",
       " 'Rennes',\n",
       " ',',\n",
       " 'France',\n",
       " 'Campus',\n",
       " 'Universitaire',\n",
       " 'de',\n",
       " 'Beaulieu',\n",
       " '35042',\n",
       " 'Rennes',\n",
       " 'Cedex',\n",
       " ',',\n",
       " 'France',\n",
       " 'laurent.amsaleg',\n",
       " '@',\n",
       " 'irisa.fr',\n",
       " 'Stphane',\n",
       " 'Girard',\n",
       " 'Equipe',\n",
       " 'MISTIS',\n",
       " ',',\n",
       " 'INRIA',\n",
       " 'Grenoble',\n",
       " ',',\n",
       " 'France',\n",
       " 'Inovalle',\n",
       " ',',\n",
       " '655',\n",
       " ',',\n",
       " 'Montbonnot',\n",
       " '38334',\n",
       " 'Saint-Ismier',\n",
       " 'Cedex',\n",
       " ',',\n",
       " 'stephane.girard',\n",
       " '@',\n",
       " 'inria.fr',\n",
       " 'France',\n",
       " 'Teddy',\n",
       " 'Furon',\n",
       " 'Equipe',\n",
       " 'LINKMEDIA',\n",
       " ',',\n",
       " 'INRIA/IRISA',\n",
       " 'Rennes',\n",
       " ',',\n",
       " 'France',\n",
       " 'Campus',\n",
       " 'Universitaire',\n",
       " 'de',\n",
       " 'Beaulieu',\n",
       " '35042',\n",
       " 'Rennes',\n",
       " 'Cedex',\n",
       " ',',\n",
       " 'France',\n",
       " 'teddy.furon',\n",
       " '@',\n",
       " 'inria.fr',\n",
       " 'Ken-ichi',\n",
       " 'Kawarabayashi',\n",
       " 'National',\n",
       " 'Institute',\n",
       " 'of',\n",
       " 'Informatics',\n",
       " ',',\n",
       " 'Japan',\n",
       " '2-1-2',\n",
       " 'Hitotsubashi',\n",
       " ',',\n",
       " 'Chiyoda-ku',\n",
       " 'Tokyo',\n",
       " '101-8430',\n",
       " ',',\n",
       " 'Japan',\n",
       " 'k_keniti',\n",
       " '@',\n",
       " 'nii.ac.jp',\n",
       " 'Oussama',\n",
       " 'Chelly',\n",
       " 'National',\n",
       " 'Institute',\n",
       " 'of',\n",
       " 'Informatics',\n",
       " ',',\n",
       " 'Japan',\n",
       " '2-1-2',\n",
       " 'Hitotsubashi',\n",
       " ',',\n",
       " 'Chiyoda-ku',\n",
       " 'Tokyo',\n",
       " '101-8430',\n",
       " ',',\n",
       " 'Japan',\n",
       " 'chelly',\n",
       " '@',\n",
       " 'nii.ac.jp',\n",
       " 'Michael',\n",
       " 'E.',\n",
       " 'Houle',\n",
       " 'National',\n",
       " 'Institute',\n",
       " 'of',\n",
       " 'Informatics',\n",
       " ',',\n",
       " 'Japan',\n",
       " '2-1-2',\n",
       " 'Hitotsubashi',\n",
       " ',',\n",
       " 'Chiyoda-ku',\n",
       " 'Tokyo',\n",
       " '101-8430',\n",
       " ',',\n",
       " 'Japan',\n",
       " 'meh',\n",
       " '@',\n",
       " 'nii.ac.jp',\n",
       " 'Michael',\n",
       " 'Nett',\n",
       " 'Google',\n",
       " ',',\n",
       " 'Japan',\n",
       " '6-10-1',\n",
       " 'Roppongi',\n",
       " ',',\n",
       " 'Minato-ku',\n",
       " 'Tokyo',\n",
       " '106-6126',\n",
       " ',',\n",
       " 'Japan',\n",
       " 'mnett',\n",
       " '@',\n",
       " 'google.com',\n",
       " 'ABSTRACT',\n",
       " 'This',\n",
       " 'paper',\n",
       " 'is',\n",
       " 'concerned',\n",
       " 'with',\n",
       " 'the',\n",
       " 'estimation',\n",
       " 'of',\n",
       " 'a',\n",
       " 'local',\n",
       " 'mea-',\n",
       " 'sure',\n",
       " 'of',\n",
       " 'intrinsic',\n",
       " 'dimensionality',\n",
       " '(',\n",
       " 'ID',\n",
       " ')',\n",
       " 'recently',\n",
       " 'proposed',\n",
       " 'by',\n",
       " 'Houle',\n",
       " '.',\n",
       " 'The',\n",
       " 'local',\n",
       " 'model',\n",
       " 'can',\n",
       " 'be',\n",
       " 'regarded',\n",
       " 'as',\n",
       " 'an',\n",
       " 'extension',\n",
       " 'of',\n",
       " 'Karger',\n",
       " 'and',\n",
       " 'Ruhls',\n",
       " 'expansion',\n",
       " 'dimension',\n",
       " 'to',\n",
       " 'a',\n",
       " 'statistical',\n",
       " 'set-',\n",
       " 'ting',\n",
       " 'in',\n",
       " 'which',\n",
       " 'the',\n",
       " 'distribution',\n",
       " 'of',\n",
       " 'distances',\n",
       " 'to',\n",
       " 'a',\n",
       " 'query',\n",
       " 'point',\n",
       " 'is',\n",
       " 'modeled',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'a',\n",
       " 'continuous',\n",
       " 'random',\n",
       " 'variable',\n",
       " '.',\n",
       " 'This',\n",
       " 'form',\n",
       " 'of',\n",
       " 'intrinsic',\n",
       " 'dimensionality',\n",
       " 'can',\n",
       " 'be',\n",
       " 'particularly',\n",
       " 'useful',\n",
       " 'in',\n",
       " 'search',\n",
       " ',',\n",
       " 'classication',\n",
       " ',',\n",
       " 'outlier',\n",
       " 'detection',\n",
       " ',',\n",
       " 'and',\n",
       " 'other',\n",
       " 'contexts',\n",
       " 'in',\n",
       " 'machine',\n",
       " 'learning',\n",
       " ',',\n",
       " 'databases',\n",
       " ',',\n",
       " 'and',\n",
       " 'data',\n",
       " 'mining',\n",
       " ',',\n",
       " 'as',\n",
       " 'it',\n",
       " 'has',\n",
       " 'been',\n",
       " 'shown',\n",
       " 'to',\n",
       " 'be',\n",
       " 'equivalent',\n",
       " 'to',\n",
       " 'a',\n",
       " 'measure',\n",
       " 'of',\n",
       " 'the',\n",
       " 'discriminative',\n",
       " 'power',\n",
       " 'of',\n",
       " 'similarity',\n",
       " 'functions',\n",
       " '.',\n",
       " 'Several',\n",
       " 'estimators',\n",
       " 'of',\n",
       " 'local',\n",
       " 'ID',\n",
       " 'are',\n",
       " 'proposed',\n",
       " 'and',\n",
       " 'analyzed',\n",
       " 'based',\n",
       " 'on',\n",
       " 'extreme',\n",
       " 'value',\n",
       " 'theory',\n",
       " ',',\n",
       " 'using',\n",
       " 'maximum',\n",
       " 'likelihood',\n",
       " 'estimation',\n",
       " '(',\n",
       " 'MLE',\n",
       " ')',\n",
       " ',',\n",
       " 'the',\n",
       " 'method',\n",
       " 'of',\n",
       " 'moments',\n",
       " '(',\n",
       " 'MoM',\n",
       " ')',\n",
       " ',',\n",
       " 'probability',\n",
       " 'weighted',\n",
       " 'moments',\n",
       " '(',\n",
       " 'PWM',\n",
       " ')',\n",
       " ',',\n",
       " 'and',\n",
       " 'regularly',\n",
       " 'varying',\n",
       " 'functions',\n",
       " '(',\n",
       " 'RV',\n",
       " ')',\n",
       " '.',\n",
       " 'An',\n",
       " 'experimental',\n",
       " 'eval-',\n",
       " 'uation',\n",
       " 'is',\n",
       " 'also',\n",
       " 'provided',\n",
       " ',',\n",
       " 'using',\n",
       " 'both',\n",
       " 'real',\n",
       " 'and',\n",
       " 'articial',\n",
       " 'data',\n",
       " '.',\n",
       " 'Categories',\n",
       " 'and',\n",
       " 'Subject',\n",
       " 'Descriptors',\n",
       " 'G.3',\n",
       " '[',\n",
       " 'Mathematics',\n",
       " 'of',\n",
       " 'Computing',\n",
       " ']',\n",
       " ':',\n",
       " 'Probability',\n",
       " 'and',\n",
       " 'Statis-',\n",
       " 'ticsDistribution',\n",
       " 'Functions',\n",
       " ';',\n",
       " 'I.2.6',\n",
       " '[',\n",
       " 'Computing',\n",
       " 'Method-',\n",
       " 'ologies',\n",
       " ']',\n",
       " ':',\n",
       " 'Articial',\n",
       " 'IntelligenceLearning',\n",
       " ',',\n",
       " 'Parameter',\n",
       " 'Learn-',\n",
       " 'ing',\n",
       " 'Permission',\n",
       " 'to',\n",
       " 'make',\n",
       " 'digital',\n",
       " 'or',\n",
       " 'hard',\n",
       " 'copies',\n",
       " 'of',\n",
       " 'all',\n",
       " 'or',\n",
       " 'part',\n",
       " 'of',\n",
       " 'this',\n",
       " 'work',\n",
       " 'for',\n",
       " 'personal',\n",
       " 'or',\n",
       " 'classroom',\n",
       " 'use',\n",
       " 'is',\n",
       " 'granted',\n",
       " 'without',\n",
       " 'fee',\n",
       " 'provided',\n",
       " 'that',\n",
       " 'copies',\n",
       " 'are',\n",
       " 'not',\n",
       " 'made',\n",
       " 'or',\n",
       " 'distributed',\n",
       " 'for',\n",
       " 'prot',\n",
       " 'or',\n",
       " 'commercial',\n",
       " 'advantage',\n",
       " 'and',\n",
       " 'that',\n",
       " 'copies',\n",
       " 'bear',\n",
       " 'this',\n",
       " 'notice',\n",
       " 'and',\n",
       " 'the',\n",
       " 'full',\n",
       " 'cita-',\n",
       " 'tion',\n",
       " 'on',\n",
       " 'the',\n",
       " 'rst',\n",
       " 'page',\n",
       " '.',\n",
       " 'Copyrights',\n",
       " 'for',\n",
       " 'components',\n",
       " 'of',\n",
       " 'this',\n",
       " 'work',\n",
       " 'owned',\n",
       " 'by',\n",
       " 'others',\n",
       " 'than',\n",
       " 'ACM',\n",
       " 'must',\n",
       " 'be',\n",
       " 'honored',\n",
       " '.',\n",
       " 'Abstracting',\n",
       " 'with',\n",
       " 'credit',\n",
       " 'is',\n",
       " 'permitted',\n",
       " '.',\n",
       " 'To',\n",
       " 'copy',\n",
       " 'otherwise',\n",
       " ',',\n",
       " 'or',\n",
       " 're-',\n",
       " 'publish',\n",
       " ',',\n",
       " 'to',\n",
       " 'post',\n",
       " 'on',\n",
       " 'servers',\n",
       " 'or',\n",
       " 'to',\n",
       " 'redistribute',\n",
       " 'to',\n",
       " 'lists',\n",
       " ',',\n",
       " 'requires',\n",
       " 'prior',\n",
       " 'specic',\n",
       " 'permission',\n",
       " 'and/or',\n",
       " 'a',\n",
       " 'fee',\n",
       " '.',\n",
       " 'Request',\n",
       " 'permissions',\n",
       " 'from',\n",
       " 'Permissions',\n",
       " '@',\n",
       " 'acm.org',\n",
       " '.',\n",
       " 'KDD',\n",
       " '15',\n",
       " 'c',\n",
       " '(',\n",
       " 'cid:13',\n",
       " ')',\n",
       " '2015',\n",
       " 'ACM',\n",
       " '.',\n",
       " 'ISBN',\n",
       " '978-1-4503-3664-2/15/08',\n",
       " '...',\n",
       " '$',\n",
       " '15.00',\n",
       " '.',\n",
       " 'DOI',\n",
       " ':',\n",
       " 'http',\n",
       " ':',\n",
       " '//dx.doi.org/10.1145/2783258.2783405',\n",
       " '.',\n",
       " 'August',\n",
       " '10-13',\n",
       " ',',\n",
       " '2015',\n",
       " ',',\n",
       " 'Sydney',\n",
       " ',',\n",
       " 'Australia',\n",
       " ',',\n",
       " 'Keywords',\n",
       " 'intrinsic',\n",
       " 'dimension',\n",
       " ',',\n",
       " 'indiscriminability',\n",
       " ',',\n",
       " 'manifold',\n",
       " 'learning',\n",
       " '1',\n",
       " '.',\n",
       " 'INTRODUCTION',\n",
       " 'In',\n",
       " 'an',\n",
       " 'attempt',\n",
       " 'to',\n",
       " 'improve',\n",
       " 'the',\n",
       " 'discriminability',\n",
       " 'of',\n",
       " 'similar-',\n",
       " 'ity',\n",
       " 'measures',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'scalability',\n",
       " 'of',\n",
       " 'methods',\n",
       " 'that',\n",
       " 'depend',\n",
       " 'on',\n",
       " 'them',\n",
       " ',',\n",
       " 'much',\n",
       " 'attention',\n",
       " 'has',\n",
       " 'been',\n",
       " 'given',\n",
       " 'in',\n",
       " 'the',\n",
       " 'areas',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " ',',\n",
       " 'databases',\n",
       " ',',\n",
       " 'and',\n",
       " 'data',\n",
       " 'mining',\n",
       " 'to',\n",
       " 'the',\n",
       " 'development',\n",
       " 'of',\n",
       " 'dimensional',\n",
       " 'reduction',\n",
       " 'techniques',\n",
       " '.',\n",
       " 'Linear',\n",
       " 'techniques',\n",
       " 'for',\n",
       " 'di-',\n",
       " 'mensionality',\n",
       " 'reduction',\n",
       " 'include',\n",
       " 'Principal',\n",
       " 'Component',\n",
       " 'Anal-',\n",
       " 'ysis',\n",
       " '(',\n",
       " 'PCA',\n",
       " ')',\n",
       " 'and',\n",
       " 'its',\n",
       " 'variants',\n",
       " '[',\n",
       " '4',\n",
       " ',',\n",
       " '24',\n",
       " ']',\n",
       " '.',\n",
       " 'Non-linear',\n",
       " 'dimension-',\n",
       " 'ality',\n",
       " 'reduction',\n",
       " 'methods',\n",
       " 'also',\n",
       " 'known',\n",
       " 'as',\n",
       " 'manifold',\n",
       " 'learn-',\n",
       " 'ing',\n",
       " 'techniques',\n",
       " 'include',\n",
       " 'Isometric',\n",
       " 'Mapping',\n",
       " '[',\n",
       " '36',\n",
       " ']',\n",
       " ',',\n",
       " 'Multi-',\n",
       " 'Dimensional',\n",
       " 'Scaling',\n",
       " '[',\n",
       " '35,37',\n",
       " ']',\n",
       " ',',\n",
       " 'Locally',\n",
       " 'Linear',\n",
       " 'Embedding',\n",
       " 'and',\n",
       " 'its',\n",
       " 'variants',\n",
       " '[',\n",
       " '30',\n",
       " ']',\n",
       " ',',\n",
       " 'and',\n",
       " 'Non-Linear',\n",
       " 'Component',\n",
       " 'Analysis',\n",
       " '[',\n",
       " '32',\n",
       " ']',\n",
       " '.',\n",
       " 'Most',\n",
       " 'reduction',\n",
       " 'techniques',\n",
       " 'require',\n",
       " 'that',\n",
       " 'a',\n",
       " 'target',\n",
       " 'dimension',\n",
       " 'be',\n",
       " 'provided',\n",
       " 'by',\n",
       " 'the',\n",
       " 'user',\n",
       " ',',\n",
       " 'although',\n",
       " 'some',\n",
       " 'attempt',\n",
       " 'to',\n",
       " 'deter-',\n",
       " 'mine',\n",
       " 'the',\n",
       " 'dimension',\n",
       " 'automatically',\n",
       " '.',\n",
       " 'Ideally',\n",
       " ',',\n",
       " 'the',\n",
       " 'supplied',\n",
       " 'di-',\n",
       " 'mension',\n",
       " 'should',\n",
       " 'depend',\n",
       " 'on',\n",
       " 'the',\n",
       " 'intrinsic',\n",
       " 'dimensionality',\n",
       " '(',\n",
       " 'ID',\n",
       " ')',\n",
       " 'of',\n",
       " 'the',\n",
       " 'data',\n",
       " '.',\n",
       " 'This',\n",
       " 'has',\n",
       " 'served',\n",
       " 'to',\n",
       " 'motivate',\n",
       " 'the',\n",
       " 'development',\n",
       " 'of',\n",
       " 'models',\n",
       " 'of',\n",
       " 'ID',\n",
       " ',',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'accurate',\n",
       " 'estimators',\n",
       " '.',\n",
       " 'Over',\n",
       " 'the',\n",
       " 'past',\n",
       " 'few',\n",
       " 'decades',\n",
       " ',',\n",
       " 'many',\n",
       " 'practical',\n",
       " 'models',\n",
       " 'of',\n",
       " 'the',\n",
       " 'intrinsic',\n",
       " 'dimensionality',\n",
       " 'of',\n",
       " 'data',\n",
       " 'sets',\n",
       " 'have',\n",
       " 'been',\n",
       " 'proposed',\n",
       " '.',\n",
       " 'Examples',\n",
       " 'include',\n",
       " 'the',\n",
       " 'previously',\n",
       " 'mentioned',\n",
       " 'Principal',\n",
       " 'Com-',\n",
       " 'ponent',\n",
       " 'Analysis',\n",
       " 'and',\n",
       " 'its',\n",
       " 'variants',\n",
       " '[',\n",
       " '4',\n",
       " ',',\n",
       " '24',\n",
       " ']',\n",
       " ',',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'several',\n",
       " 'manifold',\n",
       " 'learning',\n",
       " 'techniques',\n",
       " '[',\n",
       " '26',\n",
       " ',',\n",
       " '30',\n",
       " ',',\n",
       " '32',\n",
       " ',',\n",
       " '37',\n",
       " ']',\n",
       " '.',\n",
       " 'Topological',\n",
       " 'ap-',\n",
       " 'proaches',\n",
       " 'to',\n",
       " 'ID',\n",
       " 'estimate',\n",
       " 'the',\n",
       " 'basis',\n",
       " 'dimension',\n",
       " 'of',\n",
       " 'the',\n",
       " 'tangent',\n",
       " 'space',\n",
       " 'of',\n",
       " 'the',\n",
       " 'data',\n",
       " 'manifold',\n",
       " 'from',\n",
       " 'local',\n",
       " 'samples',\n",
       " '[',\n",
       " '5,38',\n",
       " ']',\n",
       " '.',\n",
       " 'Fractal',\n",
       " 'methods',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " 'Correlation',\n",
       " 'Dimension',\n",
       " '(',\n",
       " 'CD',\n",
       " ')',\n",
       " 'estimate',\n",
       " 'an',\n",
       " 'intrinsic',\n",
       " 'dimension',\n",
       " 'from',\n",
       " 'the',\n",
       " 'space-lling',\n",
       " 'capacity',\n",
       " 'of',\n",
       " 'the',\n",
       " 'data',\n",
       " '[',\n",
       " '6',\n",
       " ',',\n",
       " '14',\n",
       " ']',\n",
       " '.',\n",
       " 'Graph-based',\n",
       " 'methods',\n",
       " 'use',\n",
       " 'the',\n",
       " 'k-nearest',\n",
       " 'neigh-',\n",
       " 'bors',\n",
       " 'graph',\n",
       " 'along',\n",
       " 'with',\n",
       " 'density',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'estimate',\n",
       " 'ID',\n",
       " '[',\n",
       " '8',\n",
       " ']',\n",
       " '.',\n",
       " 'The',\n",
       " 'aforementioned',\n",
       " 'intrinsic',\n",
       " 'dimensionality',\n",
       " 'measures',\n",
       " 'can',\n",
       " 'be',\n",
       " 'described',\n",
       " 'as',\n",
       " 'global',\n",
       " ',',\n",
       " 'in',\n",
       " 'that',\n",
       " 'they',\n",
       " 'consider',\n",
       " 'the',\n",
       " 'dimension-',\n",
       " '29',\n",
       " 'ality',\n",
       " 'of',\n",
       " 'a',\n",
       " 'given',\n",
       " 'set',\n",
       " 'as',\n",
       " 'a',\n",
       " 'whole',\n",
       " ',',\n",
       " 'without',\n",
       " 'any',\n",
       " 'individual',\n",
       " 'object',\n",
       " 'being',\n",
       " 'given',\n",
       " 'a',\n",
       " 'special',\n",
       " 'role',\n",
       " '.',\n",
       " 'In',\n",
       " 'contrast',\n",
       " ',',\n",
       " 'local',\n",
       " 'ID',\n",
       " 'measures',\n",
       " 'are',\n",
       " 'dened',\n",
       " 'in',\n",
       " 'this',\n",
       " 'paper',\n",
       " 'as',\n",
       " 'those',\n",
       " 'that',\n",
       " 'involve',\n",
       " 'only',\n",
       " 'the',\n",
       " 'k-',\n",
       " 'nearest',\n",
       " 'neighbor',\n",
       " 'distances',\n",
       " 'of',\n",
       " 'a',\n",
       " 'specic',\n",
       " 'location',\n",
       " 'in',\n",
       " 'the',\n",
       " 'space',\n",
       " '.',\n",
       " 'Several',\n",
       " 'local',\n",
       " 'intrinsic',\n",
       " 'dimensionality',\n",
       " 'models',\n",
       " 'have',\n",
       " 'been',\n",
       " 'pro-',\n",
       " 'posed',\n",
       " 'recently',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " 'expansion',\n",
       " 'dimension',\n",
       " '(',\n",
       " 'ED',\n",
       " ')',\n",
       " '[',\n",
       " '25',\n",
       " ']',\n",
       " ',',\n",
       " 'the',\n",
       " 'generalized',\n",
       " 'expansion',\n",
       " 'dimension',\n",
       " '(',\n",
       " 'GED',\n",
       " ')',\n",
       " '[',\n",
       " '19',\n",
       " ']',\n",
       " ',',\n",
       " 'the',\n",
       " 'min-',\n",
       " 'imum',\n",
       " 'neighbor',\n",
       " 'distance',\n",
       " '(',\n",
       " 'MiND',\n",
       " ')',\n",
       " '[',\n",
       " '31',\n",
       " ']',\n",
       " ',',\n",
       " 'and',\n",
       " 'local',\n",
       " 'continuous',\n",
       " 'intrinsic',\n",
       " 'dimension',\n",
       " '(',\n",
       " 'which',\n",
       " 'we',\n",
       " 'will',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'here',\n",
       " 'as',\n",
       " 'LID',\n",
       " ')',\n",
       " '[',\n",
       " '17',\n",
       " ']',\n",
       " '.',\n",
       " 'These',\n",
       " 'models',\n",
       " 'quantify',\n",
       " 'ID',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'the',\n",
       " 'rate',\n",
       " 'at',\n",
       " 'which',\n",
       " 'the',\n",
       " 'number',\n",
       " 'of',\n",
       " 'encountered',\n",
       " 'objects',\n",
       " 'grows',\n",
       " 'as',\n",
       " 'the',\n",
       " 'considered',\n",
       " 'range',\n",
       " 'of',\n",
       " 'distances',\n",
       " 'expands',\n",
       " 'from',\n",
       " 'a',\n",
       " 'reference',\n",
       " 'location',\n",
       " '.',\n",
       " 'Local',\n",
       " 'approaches',\n",
       " 'can',\n",
       " 'be',\n",
       " 'very',\n",
       " 'useful',\n",
       " 'when',\n",
       " 'data',\n",
       " 'is',\n",
       " 'com-',\n",
       " 'posed',\n",
       " 'of',\n",
       " 'heterogeneous',\n",
       " 'manifolds',\n",
       " '.',\n",
       " 'In',\n",
       " 'addition',\n",
       " 'to',\n",
       " 'applica-',\n",
       " 'tions',\n",
       " 'in',\n",
       " 'manifold',\n",
       " 'learning',\n",
       " ',',\n",
       " 'measures',\n",
       " 'of',\n",
       " 'local',\n",
       " 'ID',\n",
       " 'have',\n",
       " 'been',\n",
       " 'used',\n",
       " 'in',\n",
       " 'the',\n",
       " 'context',\n",
       " 'of',\n",
       " 'similarity',\n",
       " 'search',\n",
       " ',',\n",
       " 'where',\n",
       " 'they',\n",
       " 'are',\n",
       " 'used',\n",
       " 'to',\n",
       " 'assess',\n",
       " 'the',\n",
       " 'complexity',\n",
       " ...]"
      ]
     },
     "execution_count": 704,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I established two lists to hold the values that I extract from the text.  This itemgetter function will check for unique values.  First, I iterate over the extracted entities and see if the objects is a nltk.tree.Tree with a \"Person\" label.  If it is, and the length is equal to 1 (first or last name only), I append that value to the list. If it's larger, I iterate of the entity tree and pull out the first value only using itemgetter.  Then, I join the values from the list and append it to the destination list.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting a list out of NLTK's standard NE chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Local Intrinsic Dimensionality Laurent Amsaleg Equipe LINKMEDIA', 'Rennes', 'Campus Universitaire', 'Rennes Cedex', 'Stphane Girard Equipe MISTIS', 'Furon Equipe LINKMEDIA', 'Oussama Chelly National Institute', 'Michael E. Houle National Institute', 'Michael Nett Google', 'Houle', 'Karger', 'Subject Descriptors', 'Parameter', 'Permissions', 'Keywords', 'Isometric Mapping', 'Linear Embedding', 'Principal', 'Analysis', 'Weibull', 'Maximum', 'Hill', 'Pr', 'Fisher', 'Tippett', 'Gnedenko', 'Given', 'Ruhls', 'J', 'M', 'Hein', 'P', 'D', 'Uniformly', 'Amsterdam Library', 'Object Images', 'Likewise', 'Faster', 'D IDMLE', 'Hein Takens', 'Dataset', 'Data', 'Secular', 'Residual Life Time', 'Cambridge University Press', 'J. Fauqueur', 'Pattern Recogn', 'Statistical Modeling', 'Extreme Values', 'Hero', 'Sys', 'Smallest Member', 'Math', 'Cambridge Phil', 'Fraga Alves', 'Portugalia Mathematica', 'Terme Maximum', 'Serie Aleatoire', 'Ann', 'Gupta', 'Audibert', 'Inlierness', 'Technical Report', 'Nett', 'Source Coding', 'Principal Component Analysis', 'Networks', 'Linear', 'Nonlinear Dimensionality Reduction', 'Locally Linear Embedding', 'Science', 'Rozza', 'Novel', 'Machine Learning Journal', 'Nonlinear Component Analysis', 'Neural Computation', 'Shaft', 'Database Syst.', 'Local Multidimensional Scaling', 'Neural Networks', 'Brunken']\n",
      "\n",
      "\n",
      "['INRIA Grenoble', 'Montbonnot', 'MLE', 'MoM', 'PWM', 'ACM', 'Principal', 'PCA', 'Component Analysis', 'ID', 'Fractal', 'Correlation Dimension', 'GED', 'MiND', 'LID', 'EVT', 'Weibull', 'Karamata', 'ED', 'CONTINUOUS', 'INTRINSIC', 'FX', 'Houle', 'IDX', 'xfX', 'EXTREME', 'Pareto', 'RV', 'GPD', 'Likelihood Estimation', 'ID2', 'ID2 X', 'Method', 'IDXk2', 'kxfX', 'Iverson', 'MiNDml1', 'max1jJ', 'IDXV', 'nFX', 'EXPERIMENTAL', 'IDRV', 'Parameters', 'CD', 'kNNG1', 'MiNDmli', 'Distance Distributions', 'Real Data', 'samplesIDMLEMoMRV', 'samplesMLEMoMRV', 'ALOI', 'MNIST', 'ANN', 'SIFT', 'PWMs', 'IDMNIST0K1K2K3K4K', 'IDANN_SIFT1B35 Dataset', 'IDMoM', 'IDRVE', 'kNNG2', 'IDMLE', 'ACKNOWLEDGMENTS', 'JST', 'JSPS Kakenhi Kiban', 'DistanceNeighbor', 'Neighbor', 'IDNeighborhood', 'Neighborhood', 'REFERENCES', 'Annals', 'Generic Image Retrieval', 'PAMI', 'Frequency Distribution', 'Largest', 'Methods of Stat.', 'Bounded Geometries', 'Density', 'Hubness', 'NII', 'Metrics', 'Document Recognition', 'IEEE', 'III', 'Kernel Eigenvalue Problem', 'ACM Trans', 'Intrinsic Dimensional Outlier']\n",
      "\n",
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "persons = []\n",
    "organizations = []\n",
    "locations =[]\n",
    "\n",
    "for l in entities:\n",
    "    if isinstance(l,nltk.tree.Tree):\n",
    "        if l.label() == 'PERSON':\n",
    "            if len(l)== 1:\n",
    "                if l[0][0] in persons:\n",
    "                    pass\n",
    "                else:\n",
    "                    persons.append(l[0][0])\n",
    "            else:\n",
    "                if \" \".join(map(itemgetter(0), l)) in persons:\n",
    "                    pass\n",
    "                else:\n",
    "                    persons.append(\" \".join(map(itemgetter(0), l)))\n",
    "                    \n",
    "for o in entities:\n",
    "    if isinstance(o,nltk.tree.Tree):\n",
    "        if o.label() == 'ORGANIZATION':\n",
    "            if len(o)== 1:\n",
    "                if o[0][0] in organizations:\n",
    "                    pass\n",
    "                else:\n",
    "                    organizations.append(o[0][0])\n",
    "            else:\n",
    "                if \" \".join(map(itemgetter(0), o)) in organizations:\n",
    "                    pass\n",
    "                else:\n",
    "                    organizations.append(\" \".join(map(itemgetter(0), o)))\n",
    "                    \n",
    "for o in entities:\n",
    "    if isinstance(o,nltk.tree.Tree):\n",
    "        if o.label() == 'LOCATION':\n",
    "            if len(o)== 1:\n",
    "                if o[0][0] in locations:\n",
    "                    pass\n",
    "                else:\n",
    "                    locations.append(o[0][0])\n",
    "            else:\n",
    "                if \" \".join(map(itemgetter(0), o)) in locations:\n",
    "                    pass\n",
    "                else:\n",
    "                    locations.append(\" \".join(map(itemgetter(0), o)))\n",
    "                    \n",
    "                \n",
    "print persons\n",
    "print\n",
    "print\n",
    "print organizations\n",
    "print\n",
    "print\n",
    "print locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INRIA Grenoble\n",
      "France Inovalle\n",
      "Component Analysis\n",
      "Correlation Dimension\n",
      "Likelihood Estimation\n",
      "ID2 X\n",
      "Distance Distributions\n",
      "Real Data\n",
      "Swiss Roll\n",
      "IDANN_SIFT1B35 Dataset\n",
      "Real Data\n",
      "JSPS Kakenhi Kiban\n",
      "Generic Image Retrieval\n",
      "Asilomar Conf\n",
      "Frequency Distribution\n",
      "Methods of Stat.\n",
      "Bounded Geometries\n",
      "Distance Distributions\n",
      "Document Recognition\n",
      "Kernel Eigenvalue Problem\n",
      "ACM Trans\n",
      "Intrinsic Dimensional Outlier\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "for o in entities:\n",
    "    if isinstance(o,nltk.tree.Tree):\n",
    "        if o.label() == 'ORGANIZATION' or o.label() == 'GPE':\n",
    "            if len(o)>1:\n",
    "                print \" \".join(map(itemgetter(0), o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to iterate over the extracted list of entities to get a better break between person's and their university name.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = [nltk.word_tokenize(l) for l in persons]\n",
    "fin = [nltk.chunk.ne_chunk(nltk.pos_tag(l)) for l in tokens]\n",
    "fin;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new =[word_tokenize(l) for l in persons]\n",
    "stan = [st.tag(l) for l in new]\n",
    "stan;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating lists of named entities from Stanford's NER model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function looks though an extracted stanford ner list, and finds continuous entitiy labels.  This should create first name, last name records of entities.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_continuous_chunks(tagged_sent):\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for token, tag in tagged_sent:\n",
    "        if tag != \"O\":\n",
    "            current_chunk.append((token, tag))\n",
    "        else:\n",
    "            if current_chunk: # if the current chunk is not empty\n",
    "                continuous_chunk.append(current_chunk)\n",
    "                current_chunk = []\n",
    "    # Flush the final current_chunk into the continuous_chunk, if any.\n",
    "    if current_chunk:\n",
    "        continuous_chunk.append(current_chunk)\n",
    "    return continuous_chunk\n",
    "\n",
    "ne_tagged_sent = [('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'), ('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'LOCATION')]\n",
    "\n",
    "named_entities = get_continuous_chunks(stanentities)\n",
    "named_entities_str = [\" \".join([token for token, tag in ne]) for ne in named_entities]\n",
    "named_entities_str_tag = [(\" \".join([token for token, tag in ne]), ne[0][1]) for ne in named_entities]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Estimating Local Intrinsic Dimensionality Laurent Amsaleg Equipe LINKMEDIA',\n",
       "  u'ORGANIZATION'),\n",
       " (u'CNRSIRISA Rennes', u'LOCATION'),\n",
       " (u'France Campus Universitaire de Beaulieu 35042 Rennes Cedex',\n",
       "  u'ORGANIZATION'),\n",
       " (u'France', u'LOCATION'),\n",
       " (u'Stphane Girard Equipe MISTIS', u'PERSON'),\n",
       " (u'INRIA Grenoble', u'LOCATION'),\n",
       " (u'France Inovalle', u'LOCATION'),\n",
       " (u'France Teddy Furon Equipe LINKMEDIA', u'ORGANIZATION'),\n",
       " (u'INRIAIRISA Rennes', u'LOCATION'),\n",
       " (u'France Campus Universitaire de Beaulieu 35042 Rennes Cedex',\n",
       "  u'ORGANIZATION'),\n",
       " (u'France', u'LOCATION'),\n",
       " (u'Kawarabayashi National Institute of Informatics', u'ORGANIZATION'),\n",
       " (u'Japan', u'LOCATION'),\n",
       " (u'Hitotsubashi', u'LOCATION'),\n",
       " (u'Chiyoda-ku Tokyo', u'LOCATION'),\n",
       " (u'Japan', u'LOCATION'),\n",
       " (u'Oussama Chelly National Institute of Informatics', u'ORGANIZATION'),\n",
       " (u'Japan', u'LOCATION'),\n",
       " (u'Hitotsubashi', u'LOCATION'),\n",
       " (u'Chiyoda-ku Tokyo', u'LOCATION'),\n",
       " (u'Japan', u'LOCATION'),\n",
       " (u'Michael E. Houle National Institute of Informatics', u'ORGANIZATION'),\n",
       " (u'Japan', u'LOCATION'),\n",
       " (u'Hitotsubashi', u'LOCATION'),\n",
       " (u'Chiyoda-ku Tokyo', u'LOCATION'),\n",
       " (u'Japan', u'LOCATION'),\n",
       " (u'Michael Nett Google', u'PERSON'),\n",
       " (u'Japan', u'LOCATION'),\n",
       " (u'Tokyo', u'LOCATION'),\n",
       " (u'Japan', u'LOCATION'),\n",
       " (u'Houle', u'PERSON'),\n",
       " (u'Karger', u'PERSON'),\n",
       " (u'MLE', u'ORGANIZATION'),\n",
       " (u'2015', u'DATE'),\n",
       " (u'$ 15.00', u'MONEY'),\n",
       " (u'August 10-13', u'DATE'),\n",
       " (u'2015', u'DATE'),\n",
       " (u'Sydney', u'LOCATION'),\n",
       " (u'Australia', u'LOCATION'),\n",
       " (u'PCA', u'ORGANIZATION'),\n",
       " (u'Weibull', u'PERSON'),\n",
       " (u'EVT', u'ORGANIZATION'),\n",
       " (u'Karamata', u'LOCATION'),\n",
       " (u'Weibull', u'PERSON'),\n",
       " (u'EVT', u'ORGANIZATION'),\n",
       " (u'Weibull', u'PERSON'),\n",
       " (u'MLE', u'ORGANIZATION'),\n",
       " (u'MLE', u'ORGANIZATION'),\n",
       " (u'MLE', u'ORGANIZATION'),\n",
       " (u'Houle', u'PERSON'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'Houle', u'PERSON'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'Fisher', u'PERSON'),\n",
       " (u'Tippett', u'PERSON'),\n",
       " (u'Balkema', u'PERSON'),\n",
       " (u'Haan', u'PERSON'),\n",
       " (u'Pickands', u'ORGANIZATION'),\n",
       " (u'Haan', u'PERSON'),\n",
       " (u'Pickands', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'Haan', u'PERSON'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'GPD', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'MLE', u'ORGANIZATION'),\n",
       " (u'Fisher', u'PERSON'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'n2k', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'Iverson', u'PERSON'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'Karger', u'PERSON'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'MLE', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'S1', u'ORGANIZATION'),\n",
       " (u'GED', u'ORGANIZATION'),\n",
       " (u'S1', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'IDX', u'ORGANIZATION'),\n",
       " (u'MLE', u'ORGANIZATION'),\n",
       " (u'PCA', u'ORGANIZATION'),\n",
       " (u'PCA', u'ORGANIZATION'),\n",
       " (u'Hein', u'PERSON'),\n",
       " (u'MLE', u'ORGANIZATION'),\n",
       " (u'3 4 5 6 7 8 9 10a 10b 10c 11 12 13 10 3 4 4 2 6 2 12 20 10 17 24 2 20 1 11',\n",
       "  u'DATE'),\n",
       " (u'Mobius', u'PERSON'),\n",
       " (u'7 8 9 10 11 12', u'DATE'),\n",
       " (u') 30 32 34 36 38 40 42 44 46', u'DATE'),\n",
       " (u'MLE', u'ORGANIZATION'),\n",
       " (u'MLE', u'ORGANIZATION'),\n",
       " (u'ALOI ( Amsterdam Library of Object Images', u'ORGANIZATION'),\n",
       " (u'ANN SIFT1B', u'PERSON'),\n",
       " (u'ANN SIFT1B', u'PERSON'),\n",
       " (u'MLE', u'ORGANIZATION'),\n",
       " (u'MLE', u'ORGANIZATION'),\n",
       " (u'PCA', u'ORGANIZATION'),\n",
       " (u'PCA', u'ORGANIZATION'),\n",
       " (u'0 5 10 15 20 25 30', u'DATE'),\n",
       " (u'0 5 10 15 20 25 30 350K2K4K6K8K10KEstimated IDQueryMNIST 0 5 10 15 20 25 30 350K2K4K6K8K10KEstimated IDQueryANN_SIFT1BABDEFCG0K1K2K3K4K 0 5 10 15 20 25 30',\n",
       "  u'DATE'),\n",
       " (u'0 5 10 15 20 25 30', u'DATE'),\n",
       " (u'0 5 10 15 20 25 30', u'DATE'),\n",
       " (u'12.29 12 12.39 20 7.39 10 24 14.05 2.49 2 20 12.48 11 5 6 3 72 20 11 25 3 20',\n",
       "  u'DATE'),\n",
       " (u'1.96 13.72 12 14.47 20 8.20 10 16.66 24 1.99 2 20 15.46 11 5 6 3 72 20 11 25 3 20',\n",
       "  u'DATE'),\n",
       " (u'44.17 12 20 -21.77 21.46 10 7.98 24 32.16 2 20 -22.83 11 5 6 3 72 20 11 25 3 20',\n",
       "  u'DATE'),\n",
       " (u'Hein Takens', u'PERSON'),\n",
       " (u'1000', u'DATE'),\n",
       " (u'Hein Takens', u'PERSON'),\n",
       " (u'Hein Takens', u'PERSON'),\n",
       " (u'-21.77 20 21.46 10 24 8.04 32.16 2 20 -22.83 11 5 6 3 72 20 11 25 3 20',\n",
       "  u'DATE'),\n",
       " (u'Hein Takens', u'PERSON'),\n",
       " (u'44.17 12 -21.77 20 21.46 10 8.04 24 31.66 2 20 -22.83 11 5 6 3 72 20 11 25 3 20',\n",
       "  u'DATE'),\n",
       " (u'Hein Takens', u'PERSON'),\n",
       " (u'PCA', u'ORGANIZATION'),\n",
       " (u'PCA', u'ORGANIZATION'),\n",
       " (u'PCA', u'ORGANIZATION'),\n",
       " (u'PCA', u'ORGANIZATION'),\n",
       " (u'EVT', u'ORGANIZATION'),\n",
       " (u'PCA', u'ORGANIZATION'),\n",
       " (u'MLE', u'ORGANIZATION'),\n",
       " (u'1000', u'DATE'),\n",
       " (u'L. Amsaleg', u'PERSON'),\n",
       " (u'T. Furon', u'ORGANIZATION'),\n",
       " (u'M. E. Houle', u'PERSON'),\n",
       " (u'K. Kawarabayashi', u'PERSON'),\n",
       " (u'JST ERATO Kawarabaya- shi Project', u'ORGANIZATION'),\n",
       " (u'M. E. Houle', u'PERSON'),\n",
       " (u'JSPS Kakenhi Kiban ( A ) Research Grant 25240036', u'ORGANIZATION'),\n",
       " (u'0 5 10 15 20 25 30 35', u'DATE'),\n",
       " (u'A. Balkema', u'PERSON'),\n",
       " (u'L. de Haan', u'PERSON'),\n",
       " (u'1974', u'DATE'),\n",
       " (u'] N. Bingham', u'PERSON'),\n",
       " (u'C. Goldie', u'ORGANIZATION'),\n",
       " (u'Cambridge University Press', u'ORGANIZATION'),\n",
       " (u'1989', u'DATE'),\n",
       " (u'] N. Boujemaa', u'PERSON'),\n",
       " (u'J. Fauqueur', u'PERSON'),\n",
       " (u'M. Ferecatu', u'PERSON'),\n",
       " (u'F. Fleuret', u'PERSON'),\n",
       " (u'V. Gouet', u'PERSON'),\n",
       " (u'B. LeSaux', u'PERSON'),\n",
       " (u'H. Sahbi', u'PERSON'),\n",
       " (u'Interactive Specic and Generic Image Retrieval', u'ORGANIZATION'),\n",
       " (u'2001', u'DATE'),\n",
       " (u'] C. Bouveyron', u'PERSON'),\n",
       " (u'G. Celeux', u'PERSON'),\n",
       " (u'S. Girard', u'PERSON'),\n",
       " (u'] J. Bruske', u'PERSON'),\n",
       " (u'G. Sommer', u'PERSON'),\n",
       " (u'PAMI', u'ORGANIZATION'),\n",
       " (u'] F. Camastra', u'ORGANIZATION'),\n",
       " (u'A. Vinciarelli', u'PERSON'),\n",
       " (u'PAMI', u'ORGANIZATION'),\n",
       " (u'] S. Coles', u'PERSON'),\n",
       " (u'Statistical Modeling of Extreme Values', u'ORGANIZATION'),\n",
       " (u'] J. Costa', u'ORGANIZATION'),\n",
       " (u'Asilomar Conf', u'ORGANIZATION'),\n",
       " (u'2003', u'DATE'),\n",
       " (u'T. de Vries', u'ORGANIZATION'),\n",
       " (u'S. Chawla', u'PERSON'),\n",
       " (u'M. E. Houle', u'PERSON'),\n",
       " (u'2010', u'DATE'),\n",
       " (u'] R. A. Fisher', u'PERSON'),\n",
       " (u'L. H. C. Tippett', u'PERSON'),\n",
       " (u'Cambridge Phil', u'LOCATION'),\n",
       " (u'1928', u'DATE'),\n",
       " (u'] M. I. Fraga Alves', u'PERSON'),\n",
       " (u'L. de Haan', u'PERSON'),\n",
       " (u'T. Lin', u'PERSON'),\n",
       " (u'] M. I. Fraga Alves', u'PERSON'),\n",
       " (u'M. I. Gomes', u'PERSON'),\n",
       " (u'L. de Haan', u'PERSON'),\n",
       " (u'Portugalia Mathematica', u'ORGANIZATION'),\n",
       " (u'2003', u'DATE'),\n",
       " (u'B. V. Gnedenko', u'PERSON'),\n",
       " (u'Distribution Limite du Terme Maximum', u'ORGANIZATION'),\n",
       " (u'1943', u'DATE'),\n",
       " (u'A. Gupta', u'PERSON'),\n",
       " (u'R. Krauthgamer', u'PERSON'),\n",
       " (u'J. R. Lee', u'PERSON'),\n",
       " (u'2003', u'DATE'),\n",
       " (u'] M. Hein', u'PERSON'),\n",
       " (u'2005', u'DATE'),\n",
       " (u'B. M. Hill', u'PERSON'),\n",
       " (u'1975', u'DATE'),\n",
       " (u'M. E. Houle', u'PERSON'),\n",
       " (u'Discriminability , Density & Distance Distributions', u'ORGANIZATION'),\n",
       " (u'2013', u'DATE'),\n",
       " (u'M. E. Houle', u'PERSON'),\n",
       " (u'Outlierness', u'LOCATION'),\n",
       " (u'Hubness', u'PERSON'),\n",
       " (u'Extreme-Value-Theoretic Foundation', u'ORGANIZATION'),\n",
       " (u'NII', u'ORGANIZATION'),\n",
       " (u'2015', u'DATE'),\n",
       " (u'M. E. Houle', u'PERSON'),\n",
       " (u'H. Kashima', u'PERSON'),\n",
       " (u'M. Nett', u'PERSON'),\n",
       " (u'2012', u'DATE'),\n",
       " (u'M. E. Houle', u'PERSON'),\n",
       " (u'X. Ma', u'PERSON'),\n",
       " (u'M. Nett', u'PERSON'),\n",
       " (u'V. Oria', u'PERSON'),\n",
       " (u'Multi-Step Similarity Search', u'ORGANIZATION'),\n",
       " (u'2012', u'DATE'),\n",
       " (u'M. E. Houle', u'PERSON'),\n",
       " (u'X. Ma', u'PERSON'),\n",
       " (u'V. Oria', u'PERSON'),\n",
       " (u'2014', u'DATE'),\n",
       " (u'M. E. Houle', u'PERSON'),\n",
       " (u'M. Nett', u'PERSON'),\n",
       " (u'PAMI', u'ORGANIZATION'),\n",
       " (u'2015', u'DATE'),\n",
       " (u'] H. Jegou', u'PERSON'),\n",
       " (u'R. Tavenard', u'PERSON'),\n",
       " (u'M. Douze', u'PERSON'),\n",
       " (u'L. Amsaleg', u'PERSON'),\n",
       " (u'Source Coding', u'ORGANIZATION'),\n",
       " (u'2011', u'DATE'),\n",
       " (u'I. Jollie', u'PERSON'),\n",
       " (u'1986', u'DATE'),\n",
       " (u'] D. R. Karger', u'PERSON'),\n",
       " (u'M. Ruhl', u'PERSON'),\n",
       " (u'2002', u'DATE'),\n",
       " (u'J. Karhunen', u'PERSON'),\n",
       " (u'J. Joutsensalo', u'PERSON'),\n",
       " (u'PCA', u'ORGANIZATION'),\n",
       " (u'Neural Networks', u'ORGANIZATION'),\n",
       " (u'1994', u'DATE'),\n",
       " (u'] Y. LeCun', u'PERSON'),\n",
       " (u'L. Bottou', u'PERSON'),\n",
       " (u'Y. Bengio', u'PERSON'),\n",
       " (u'P. Haner', u'PERSON'),\n",
       " (u'1998', u'DATE'),\n",
       " (u'J. Pickands', u'ORGANIZATION'),\n",
       " (u'Statistical Inference Using Extreme Order Statistics', u'ORGANIZATION'),\n",
       " (u'1975', u'DATE'),\n",
       " (u'C. R. Rao', u'PERSON'),\n",
       " (u'1973', u'DATE'),\n",
       " (u'S. T. Roweis', u'PERSON'),\n",
       " (u'L. K. Saul', u'PERSON'),\n",
       " (u'2000', u'DATE'),\n",
       " (u'A. Rozza', u'PERSON'),\n",
       " (u'G. Lombardi', u'PERSON'),\n",
       " (u'C. Ceruti', u'PERSON'),\n",
       " (u'E. Casiraghi', u'PERSON'),\n",
       " (u'P. Campadelli', u'PERSON'),\n",
       " (u'2012', u'DATE'),\n",
       " (u'B. Scholkopf', u'PERSON'),\n",
       " (u'A. J. Smola', u'PERSON'),\n",
       " (u'K.-R. Muller', u'PERSON'),\n",
       " (u'Kernel Eigenvalue Problem', u'ORGANIZATION'),\n",
       " (u'1998', u'DATE'),\n",
       " (u'U. Shaft', u'PERSON'),\n",
       " (u'R. Ramakrishnan', u'PERSON'),\n",
       " (u'2006', u'DATE'),\n",
       " (u'] F. Takens', u'ORGANIZATION'),\n",
       " (u'1985', u'DATE'),\n",
       " (u'J. Tenenbaum', u'PERSON'),\n",
       " (u'V. D. Silva', u'PERSON'),\n",
       " (u'J. Langford', u'PERSON'),\n",
       " (u'2000', u'DATE'),\n",
       " (u'B. Tenenbaum', u'PERSON'),\n",
       " (u'V. De Silva', u'ORGANIZATION'),\n",
       " (u'J. C. Langford', u'PERSON'),\n",
       " (u'2000', u'DATE'),\n",
       " (u'J. Venna', u'PERSON'),\n",
       " (u'S. Kaski', u'PERSON'),\n",
       " (u'Neural Networks', u'ORGANIZATION'),\n",
       " (u'2006', u'DATE'),\n",
       " (u'P. Verveer', u'PERSON'),\n",
       " (u'R. Duin', u'PERSON'),\n",
       " (u'PAMI', u'ORGANIZATION'),\n",
       " (u'1995', u'DATE'),\n",
       " (u'] J. von Brunken', u'PERSON'),\n",
       " (u'M. E. Houle', u'PERSON'),\n",
       " (u'A. Zimek', u'PERSON'),\n",
       " (u'High-Dimensional Data', u'ORGANIZATION'),\n",
       " (u'NII', u'ORGANIZATION'),\n",
       " (u'2015', u'DATE')]"
      ]
     },
     "execution_count": 711,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "named_entities = get_continuous_chunks(stanentities)\n",
    "named_entities_str = [\" \".join([token for token, tag in ne]) for ne in named_entities]\n",
    "named_entities_str_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stphane Girard Equipe MISTIS\n",
      "Michael Nett Google\n",
      "Houle\n",
      "Karger\n",
      "Weibull\n",
      "Weibull\n",
      "Weibull\n",
      "Houle\n",
      "Houle\n",
      "Fisher\n",
      "Tippett\n",
      "Balkema\n",
      "Haan\n",
      "Haan\n",
      "Haan\n",
      "Fisher\n",
      "Iverson\n",
      "Karger\n",
      "Hein\n",
      "Mobius\n",
      "ANN SIFT1B\n",
      "ANN SIFT1B\n",
      "Hein Takens\n",
      "Hein Takens\n",
      "Hein Takens\n",
      "Hein Takens\n",
      "Hein Takens\n",
      "L. Amsaleg\n",
      "M. E. Houle\n",
      "K. Kawarabayashi\n",
      "M. E. Houle\n",
      "A. Balkema\n",
      "L. de Haan\n",
      "] N. Bingham\n",
      "] N. Boujemaa\n",
      "J. Fauqueur\n",
      "M. Ferecatu\n",
      "F. Fleuret\n",
      "V. Gouet\n",
      "B. LeSaux\n",
      "H. Sahbi\n",
      "] C. Bouveyron\n",
      "G. Celeux\n",
      "S. Girard\n",
      "] J. Bruske\n",
      "G. Sommer\n",
      "A. Vinciarelli\n",
      "] S. Coles\n",
      "S. Chawla\n",
      "M. E. Houle\n",
      "] R. A. Fisher\n",
      "L. H. C. Tippett\n",
      "] M. I. Fraga Alves\n",
      "L. de Haan\n",
      "T. Lin\n",
      "] M. I. Fraga Alves\n",
      "M. I. Gomes\n",
      "L. de Haan\n",
      "B. V. Gnedenko\n",
      "A. Gupta\n",
      "R. Krauthgamer\n",
      "J. R. Lee\n",
      "] M. Hein\n",
      "B. M. Hill\n",
      "M. E. Houle\n",
      "M. E. Houle\n",
      "Hubness\n",
      "M. E. Houle\n",
      "H. Kashima\n",
      "M. Nett\n",
      "M. E. Houle\n",
      "X. Ma\n",
      "M. Nett\n",
      "V. Oria\n",
      "M. E. Houle\n",
      "X. Ma\n",
      "V. Oria\n",
      "M. E. Houle\n",
      "M. Nett\n",
      "] H. Jegou\n",
      "R. Tavenard\n",
      "M. Douze\n",
      "L. Amsaleg\n",
      "I. Jollie\n",
      "] D. R. Karger\n",
      "M. Ruhl\n",
      "J. Karhunen\n",
      "J. Joutsensalo\n",
      "] Y. LeCun\n",
      "L. Bottou\n",
      "Y. Bengio\n",
      "P. Haner\n",
      "C. R. Rao\n",
      "S. T. Roweis\n",
      "L. K. Saul\n",
      "A. Rozza\n",
      "G. Lombardi\n",
      "C. Ceruti\n",
      "E. Casiraghi\n",
      "P. Campadelli\n",
      "B. Scholkopf\n",
      "A. J. Smola\n",
      "K.-R. Muller\n",
      "U. Shaft\n",
      "R. Ramakrishnan\n",
      "J. Tenenbaum\n",
      "V. D. Silva\n",
      "J. Langford\n",
      "B. Tenenbaum\n",
      "J. C. Langford\n",
      "J. Venna\n",
      "S. Kaski\n",
      "P. Verveer\n",
      "R. Duin\n",
      "] J. von Brunken\n",
      "M. E. Houle\n",
      "A. Zimek\n"
     ]
    }
   ],
   "source": [
    "for l,m in named_entities_str_tag:\n",
    "    if m == 'PERSON':\n",
    "        print l\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list1 = range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list2 = [i for i in xrange(7,17,1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{7, 8, 9}"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(list1) & set(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parts_of_speech(corpus):\n",
    "    \"returns named entity chunks in a given text\"\n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(corpus))\n",
    "    entities = nltk.chunk.ne_chunk(tagged)\n",
    "    # Another entity extractor\n",
    "    st = StanfordNERTagger('/Users/linwood/stanford-corenlp-full-2015-04-20/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "           '/Users/linwood/stanford-corenlp-full-2015-04-20/stanford-corenlp-3.5.2.jar',\n",
    "           encoding='utf-8')\n",
    "    tokenized_text = word_tokenize(corpus)\n",
    "    stanentities = st.tag(tokenized_text)\n",
    "    return entities\n",
    "def find_entities(chunks):\n",
    "    \"given list of tagged parts of speech, returns unique named entities\"\n",
    "\n",
    "    def traverse(tree):\n",
    "        \"recursively traverses an nltk.tree.Tree to find named entities\"\n",
    "        entity_names = []\n",
    "    \n",
    "        if hasattr(tree, 'node') and tree.node:\n",
    "            if tree.node == 'NE':\n",
    "                entity_names.append(' '.join([child[0] for child in tree]))\n",
    "            else:\n",
    "                for child in tree:\n",
    "                    entity_names.extend(traverse(child))\n",
    "    \n",
    "        return entity_names\n",
    "    \n",
    "    named_entities = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        entities = sorted(list(set([word for tree in chunk\n",
    "                            for word in traverse(tree)])))\n",
    "        for e in entities:\n",
    "            if e not in named_entities:\n",
    "                named_entities.append(e)\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/linwood/projects/LC3-Creations/notebooks'"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting entities and creating lists using Polyglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from polyglot.text import Text\n",
    "e=Text(re.sub('[\\s]',\" \",document[:10000])).entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Text(filter(lambda x: x in string.printable, document)).entities;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# problem with unicode; have to get rid of this somehow or the extraction errors out.\n",
    "# solution from http://stackoverflow.com/questions/8689795/how-can-i-remove-non-ascii-characters-but-leave-periods-and-spaces-using-python\n",
    "\n",
    "import string\n",
    "s=document\n",
    "Text(filter(lambda x: x in string.printable, s)).entities;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# when I get an error for some unicode character, try to get text up to the error; will need a \"while\" loop\n",
    "\n",
    "Text(re.sub('[\\s]',\" \",(document[len(document)-(len(document)/10):len(document)]))).entities;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Text(re.sub('[\\s]',\" \",document[:2000])).entities;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code iterates over the polyglot extracted entities and creates a list of person, locations, and organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import unicodedata\n",
    "\n",
    "def extraction(corpus):\n",
    "    \n",
    "    # extract entities from a single string; remove whitespace characters\n",
    "    try:\n",
    "        e = Text(re.sub('[\\s]',\" \",corpus)).entities\n",
    "    except:\n",
    "        pass #e = Text(re.sub(\"(r'(x0)',\" \",\"(re.sub('[\\s]',\" \",corpus)))).entities\n",
    "    \n",
    "    current_person =[]\n",
    "    persons =[]\n",
    "    current_org=[]\n",
    "    organizations=[]\n",
    "    current_loc=[]\n",
    "    locations=[]\n",
    "\n",
    "    for l in e:\n",
    "        if l.tag == 'I-PER':\n",
    "            for m in l:\n",
    "                current_person.append(unicodedata.normalize('NFKD', m).encode('ascii','ignore'))\n",
    "            else:\n",
    "                    if current_person: # if the current chunk is not empty\n",
    "                        persons.append(\" \".join(current_person))\n",
    "                        current_person = []\n",
    "        elif l.tag == 'I-ORG':\n",
    "            for m in l:\n",
    "                current_org.append(unicodedata.normalize('NFKD', m).encode('ascii','ignore'))\n",
    "            else:\n",
    "                    if current_org: # if the current chunk is not empty\n",
    "                        organizations.append(\" \".join(current_org))\n",
    "                        current_org = []\n",
    "        elif l.tag == 'I-LOC':\n",
    "            for m in l:\n",
    "                current_loc.append(unicodedata.normalize('NFKD', m).encode('ascii','ignore'))\n",
    "            else:\n",
    "                    if current_loc: # if the current chunk is not empty\n",
    "                        locations.append(\" \".join(current_loc))\n",
    "                        current_loc = []\n",
    "    results = {}\n",
    "    results['persons']=persons\n",
    "    results['organizations']=organizations\n",
    "    results['locations']=locations\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Laurent',\n",
       " 'Equipe',\n",
       " 'Stphane Girard Equipe MISTIS',\n",
       " 'Ismier',\n",
       " 'Teddy Furon Equipe',\n",
       " 'IRISA',\n",
       " 'ichi',\n",
       " 'Michael E',\n",
       " 'Michael Nett Google',\n",
       " 'Com',\n",
       " 'estima',\n",
       " 'Fisher',\n",
       " 'Tippett',\n",
       " 'de Haan',\n",
       " 'de Haan',\n",
       " 'Fisher',\n",
       " 'Tippett',\n",
       " 'Theo',\n",
       " 'Fisher',\n",
       " 'Tippett',\n",
       " 'satises lim',\n",
       " 'Fisher',\n",
       " 'ned',\n",
       " 'ID2',\n",
       " 'Iverson',\n",
       " 'IDX',\n",
       " 'Sa',\n",
       " 'Hein',\n",
       " 'MLE',\n",
       " 'ber',\n",
       " 'benet',\n",
       " 'FrequencyEstimated',\n",
       " 'FrequencyEstimated',\n",
       " 'Hein',\n",
       " 'IDMoM',\n",
       " 'Hein',\n",
       " 'IDMoM IDPWM',\n",
       " 'MiNDml1',\n",
       " 'Hein Takens',\n",
       " 'MiNDml1',\n",
       " 'Hein Takens',\n",
       " 'MiNDml1',\n",
       " 'Hein Takens',\n",
       " 'signicantly',\n",
       " 'ACKNOWLEDGMENTS',\n",
       " 'Furon',\n",
       " 'Kawarabayashi',\n",
       " 'JST ERATO',\n",
       " 'Houle',\n",
       " 'JSPS Kakenhi',\n",
       " 'Grant',\n",
       " 'de Haan',\n",
       " 'Bingham',\n",
       " 'Goldie',\n",
       " 'Teugels',\n",
       " 'Fauqueur',\n",
       " 'Sommer',\n",
       " 'Coles',\n",
       " 'Costa',\n",
       " 'de Vries',\n",
       " 'Chawla',\n",
       " '. Fisher',\n",
       " 'Tippett',\n",
       " 'Cambridge Phil',\n",
       " 'Soc',\n",
       " 'Fraga Alves',\n",
       " 'de Haan',\n",
       " 'Fraga Alves',\n",
       " 'Gomes',\n",
       " 'de Haan',\n",
       " 'Ann .',\n",
       " 'Gupta',\n",
       " 'Lee',\n",
       " 'Hein',\n",
       " 'J',\n",
       " '. Stat',\n",
       " 'Jollie',\n",
       " 'Karhunen',\n",
       " 'Pickands',\n",
       " '. Stat',\n",
       " 'Rao',\n",
       " 'Saul',\n",
       " 'Lombardi',\n",
       " 'Campadelli',\n",
       " 'Smola',\n",
       " 'Muller',\n",
       " 'indexability',\n",
       " '. Silva',\n",
       " 'Langford',\n",
       " 'De Silva',\n",
       " 'Langford',\n",
       " 'von Brunken']"
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction(document)['persons']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regexp = re.compile(\"REFERENCES(.*)$\")\n",
    "references = Text(regexp.search(re.sub('[\\s]',\" \",document)).group(1)).entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regexp1 = re.compile(\"REFERENCES(.*)$\")\n",
    "references = Text(regexp.search(re.sub('[\\s]',\" \",document)).group(1)).entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extraction(regexp.search(re.sub('[\\s]',\" \",document)).group(1))['persons'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completed Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting the number of references in a research paper\n",
    "\n",
    "len(re.findall('\\[(.*?)\\]',regexp.search(re.sub('[\\s]',\" \",document)).group(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truth Sets to test extraction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 authors\n",
      "\n",
      "There are 3 author organizations\n",
      "\n",
      "There are 7 author locations\n",
      "\n",
      "There are 152 authors in the references\n"
     ]
    }
   ],
   "source": [
    "#p19.pdf\n",
    "\n",
    "p19pdf_authors=['Tim Althoff','Xin Luna Dong','Kevin Murphy','Safa Alai','Van Dang','Wei Zhang']\n",
    "p19pdf_author_organizations=['Computer Science Department','Stanford University','Google']\n",
    "p19pdf_author_locations=['Stanford, CA','Stanford','CA','Google','1600 Amphitheatre Parkway, Mountain View, CA 94043','1600 Amphitheatre Parkway','Mountain View']\n",
    "\n",
    "p19pdf_references_authors =['A. Ahmed', 'C. H. Teo', 'S. Vishwanathan','A. Smola','J. Allan', 'R. Gupta', 'V. Khandelwal',\n",
    "                           'D. Graus', 'M.-H. Peetz', 'D. Odijk', 'O. de Rooij', 'M. de Rijke','T. Huet', 'J. Biega', \n",
    "                            'F. M. Suchanek','H. Ji', 'T. Cassidy', 'Q. Li','S. Tamang', 'A. Kannan', 'S. Baker', 'K. Ramnath', \n",
    "                            'J. Fiss', 'D. Lin', 'L. Vanderwende',  'R. Ansary', 'A. Kapoor', 'Q. Ke', 'M. Uyttendaele',\n",
    "                           'S. M. Katz','A. Krause','D. Golovin','J. Leskovec', 'A. Krause', 'C. Guestrin', 'C. Faloutsos', \n",
    "                            'J. VanBriesen','N. Glance','J. Li','C. Cardie','J. Li','C. Cardie','C.-Y. Lin','H. Lin','J. A. Bilmes'\n",
    "                           'X. Ling','D. S. Weld', 'A. Mazeika', 'T. Tylenda','G. Weikum','M. Minoux', 'G. L. Nemhauser', 'L. A. Wolsey',\n",
    "                            'M. L. Fisher','R. Qian','D. Shahaf', 'C. Guestrin','E. Horvitz','T. Althoff', 'X. L. Dong', 'K. Murphy', 'S. Alai',\n",
    "                            'V. Dang','W. Zhang','R. A. Baeza-Yates', 'B. Ribeiro-Neto', 'D. Shahaf', 'J. Yang', 'C. Suen', 'J. Jacobs', 'H. Wang', 'J. Leskovec',\n",
    "                           'W. Shen', 'J. Wang', 'J. Han','D. Bamman', 'N. Smith','K. Bollacker', 'C. Evans', 'P. Paritosh', 'T. Sturge', 'J. Taylor',\n",
    "                           'R. Sipos', 'A. Swaminathan', 'P. Shivaswamy', 'T. Joachims','K. Sprck Jones','G. Calinescu', 'C. Chekuri', 'M. Pl','J. Vondrk',\n",
    "                           'F. M. Suchanek', 'G. Kasneci','G. Weikum', 'J. Carbonell' ,'J. Goldstein','B. Carterette', 'P. N. Bennett', 'D. M. Chickering',\n",
    "                            'S. T. Dumais','A. Dasgupta', 'R. Kumar','S. Ravi','Q. X. Do', 'W. Lu', 'D. Roth','X. Dong', 'E. Gabrilovich', 'G. Heitz', 'W. Horn', \n",
    "                            'N. Lao', 'K. Murphy',  'T. Strohmann', 'S. Sun','W. Zhang', 'M. Dubinko', 'R. Kumar', 'J. Magnani', 'J. Novak', 'P. Raghavan','A. Tomkins',\n",
    "                           'U. Feige','F. M. Suchanek','N. Preda','R. Swan','J. Allan', 'T. Tran', 'A. Ceroni', 'M. Georgescu', 'K. D. Naini', 'M. Fisichella',\n",
    "                           'T. A. Tuan', 'S. Elbassuoni', 'N. Preda','G. Weikum','Y. Wang', 'M. Zhu', 'L. Qu', 'M. Spaniol', 'G. Weikum',\n",
    "                           'G. Weikum', 'N. Ntarmos', 'M. Spaniol', 'P. Triantallou', 'A. A. Benczr',  'S. Kirkpatrick', 'P. Rigaux','M. Williamson',\n",
    "                           'X. W. Zhao', 'Y. Guo', 'R. Yan', 'Y. He','X. Li']\n",
    "\n",
    "\n",
    "print \"There are %r authors\" % len(p19pdf_authors)\n",
    "print  # white space\n",
    "print \"There are %r author organizations\" %len(p19pdf_author_organizations)\n",
    "print \n",
    "print \"There are %r author locations\" % len(p19pdf_author_locations)\n",
    "print  \n",
    "print \"There are %r authors in the references\" %len(p19pdf_references_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 authors\n",
      "\n",
      "There are 6 author organizations\n",
      "\n",
      "There are 8 author locations\n",
      "\n",
      "There are 106 authors in the references\n"
     ]
    }
   ],
   "source": [
    "#p29.pdf\n",
    "\n",
    "p29pdf_authors=['Laurent Amsaleg','Stephane Girard','Oussama Chelly','Teddy Furon','Michael E. Houle','Ken-ichi Kawarabayashi',\n",
    "               'Michael Nett']\n",
    "p29pdf_author_organizations=['Equipe LINKMEDIA','Campus Universitaire de Beaulieu','CNRS/IRISA Rennes','National Institute of Informatics',\n",
    "                             'Equipe MISTIS INRIA','Google']\n",
    "p29pdf_author_locations=['Campus Universitaire de Beaulieu','35042 Rennes Cedex, France','France','-1-2 Hitotsubashi, Chiyoda-ku Tokyo 101-8430, Japan',\n",
    "                        'Japan','6-10-1 Roppongi, Minato-ku Tokyo 106-6126','Inovallee, 655, Montbonnot 38334 Saint-Ismier Cedex','Tokyo']\n",
    "\n",
    "p29pdf_references_authors =['A. A. Balkema','L. de Haan','N. Bingham', 'C. Goldie','J. Teugels','N. Boujemaa', 'J. Fauqueur', 'M. Ferecatu', 'F. Fleuret',\n",
    "                            'V. Gouet', 'B. LeSaux','H. Sahbi','C. Bouveyron', 'G. Celeux', 'S. Girard','J. Bruske', 'G. Sommer',\n",
    "                           'F. Camastra','A. Vinciarelli','S. Coles','J. Costa' ,'A. Hero','T. de Vries', 'S. Chawla','M. E. Houle',\n",
    "                           'R. A. Fisher','L. H. C. Tippett','M. I. Fraga Alves', 'L. de Haan','T. Lin','M. I. Fraga Alves', 'M. I. Gomes','L. de Haan',\n",
    "                           'B. V. Gnedenko',' A. Gupta', 'R. Krauthgamer','J. R. Lee','A. Gupta', 'R. Krauthgamer','J. R. Lee','M. Hein','J.-Y. Audibert',\n",
    "                           'B. M. Hill','M. E. Houle','M. E. Houle','M. E. Houle','M. E. Houle', 'H. Kashima', 'M. Nett','M. E. Houle', 'X. Ma', 'M. Nett',\n",
    "                            'V. Oria','M. E. Houle', 'X. Ma', 'V. Oria','J. Sun','M. E. Houle','M. Nett','H. Jegou', 'R. Tavenard', 'M. Douze','L. Amsaleg',\n",
    "                           'I. Jollie','D. R. Karger','M. Ruhl','J. Karhunen','J. Joutsensalo','Y. LeCun', 'L. Bottou', 'Y. Bengio', 'P. Haner',\n",
    "                           'J. Pickands, III','C. R. Rao','S. T. Roweis','L. K. Saul','A. Rozza', 'G. Lombardi', 'C. Ceruti', 'E. Casiraghi', 'P. Campadelli',\n",
    "                           'B. Scholkopf', 'A. J. Smola','K.-R. Muller','U. Shaft','R. Ramakrishnan',' F. Takens','J. Tenenbaum', 'V. D. Silva','J. Langford',\n",
    "                           'J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. B. Tenenbaum', 'V. De Silva','J. C. Langford','J. Venna','S. Kaski',\n",
    "                           'P. Verveer','R. Duin','J. von Brunken', 'M. E. Houle', 'A. Zimek','J. von Brunken', 'M. E. Houle','A. Zimek']\n",
    "\n",
    "print \"There are %r authors\" % len(p29pdf_authors)\n",
    "print  # white space\n",
    "print \"There are %r author organizations\" %len(p29pdf_author_organizations)\n",
    "print \n",
    "print \"There are %r author locations\" % len(p29pdf_author_locations)\n",
    "print  \n",
    "print \"There are %r authors in the references\" %len(p29pdf_references_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in which we compare our estimators with state-of-the-art global and local esti- mators. We also show that the empirical variance and convergence rates of the MLE (Hill) and MoM estima- tors are superior to those of the other local estimators studied.   experiments showing that local estimators are more ro- bust than global ones in the presence of noise in non- linear manifolds. Our experiments show that our ap- proaches are very competitive in this regard with other methods, both local and global.   proles of several real-world data sets in terms of LID, illustrating the degree of variability of complexity from region to region within a dataset. The proles demon- strate that a single global ID value is in general not sucient to fully characterize the complexity of real- world data.  2. CONTINUOUS INTRINSIC DIMENSION LID [17] aims to quantify the local ID of a feature space exclusively in terms of the distribution of inter-point dis- tances. Formally, let (Rm, d) be a domain equipped with a non-negative distance function d. Let us consider the distribution of distances within the domain with respect to some xed point of reference. We model this distribution in terms of a random vari'"
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('[\\s]',\" \",document)[9300:10500]\n",
    "#regexp.search(re.sub('[\\s]',\" \",document)).group(1)[4900:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'e' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-678-83f3102640a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mextraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-668-156bc50c86cc>\u001b[0m in \u001b[0;36mextraction\u001b[0;34m(corpus)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mlocations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'I-PER'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'e' referenced before assignment"
     ]
    }
   ],
   "source": [
    "extraction(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class entities(object):\n",
    "  def __init__(self):\n",
    "    self.persons = extraction(document)['persons']\n",
    "    self.organizations = extraction(document)['organizations']\n",
    "\n",
    "my_shape = entities()\n",
    "print (my_shape.persons)\n",
    "print(my_shape.organizations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import emailextractor\n",
    "from emailextractor import file_to_str, get_emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tuple(get_emails(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
